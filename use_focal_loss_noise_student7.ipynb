{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsnYZnK6FhCW"
      },
      "outputs": [],
      "source": [
        "#2. epoch가 충분하지 않다고 생각해서 30 - > 100으로 증가시켰다. (1 epoch란 모든 train-set을 1번 도는 것을 의미한다.)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "CFG = {\n",
        "    'IMG_SIZE':224,\n",
        "    'EPOCHS':100,\n",
        "    'LEARNING_RATE':5e-4,\n",
        "    'BATCH_SIZE':32,\n",
        "    'SEED':41\n",
        "}\n",
        "#3. 도배 이미지 확인 (이미지 증강을 어떤 방식으로 시키면 좋을지 고민하기위해 실제 이미지를 확인한다.\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "for idx, i in enumerate(train.label.unique()):\n",
        "    plt.subplot(4, 7, idx+1)\n",
        "\n",
        "    df = train[train['label'] == i].reset_index(drop = True)\n",
        "    # image_path = df.loc[random.randint(0, len(df))-1, 'path']\n",
        "    image_path = df.loc[random.randint(0, len(df)-1), 'img_path']\n",
        "    img = Image.open(image_path)\n",
        "    img = img.resize((224,224))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(i)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#위에 있는 번호는 클래스를 라벨 인코딩 한것을 의미하며 각각의 사진은 해당 클래스에 해당하는 하자 이미지이다.\n",
        "\n",
        "\n",
        "\n",
        "#4. 이미지를 보고 대회 talk 게시판을 참고한 결과 이미지 증강기법 중 걸레받이수정'과 '꼬임', 그리고 '몰딩수정' 데이터를 rotate 또는 verticalflip을 하면 구분을 못하게 된다는 것을 알게되었고 이를 제외하기로 했다.\n",
        "\n",
        "train_transforefv2 = A.Compose([\n",
        "    A.RandomResizedCrop(384,384, scale=(0.2, 0.8)),\n",
        "    A.Transpose(p=0.5),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    #A.VerticalFlip(p=0.5),  (제외)\n",
        "    #A.ShiftScaleRotate(p=0.5),  (제외)\n",
        "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "    A.ChannelShuffle(),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2(),\n",
        "    ])\n",
        "test_transforefv2 = A.Compose([\n",
        "                            A.Resize(384, 384),\n",
        "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "                            ToTensorV2()\n",
        "                            ])\n",
        "train_datasetefv2 = CustomDataset(train['img_path'].values, train['label'].values, train_transforefv2)\n",
        "train_loaderefv2 = DataLoader(train_datasetefv2, batch_size = CFG['BATCH_SIZE'], sampler=sampler,shuffle=False, num_workers=0)\n",
        "\n",
        "val_datasetefv2 = CustomDataset(val['img_path'].values, val['label'].values, test_transforefv2)\n",
        "val_loaderefv2 = DataLoader(val_datasetefv2, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "#5. 클래스의 분포를 알아보았다. 결과를 확인하니 상당히 클래스가 불균등하다는 것을 알 수 있다.\n",
        "\n",
        "df['label'].value_counts()\n",
        "\n",
        "\t훼손\t\t\t\t\t1405\n",
        "\t오염\t\t\t\t\t595\n",
        "\t걸레받이수정\t307\n",
        "\t꼬임\t\t\t\t\t210\n",
        "\t터짐\t\t\t\t\t162\n",
        "\t곰팡이\t\t\t\t145\n",
        "\t오타공\t\t\t\t142\n",
        "\t몰딩수정\t\t\t130\n",
        "\t면불량\t\t\t\t99\n",
        "\t석고수정\t\t\t57\n",
        "\t들뜸\t\t\t\t\t54\n",
        "\t피스\t\t\t\t\t51\n",
        "\t창틀,문틀수정\t27\n",
        "\t울음\t\t\t\t\t22\n",
        "\t이음부불량\t\t17\n",
        "\t녹오염\t\t\t\t14\n",
        "\t가구수정\t\t\t12\n",
        "\t틈새과다\t\t\t5\n",
        "\t반점\t\t\t\t\t3\n",
        "#6. 이를 해결하기 위해 부족한 클래스에 가중치를 더 부여해서 학습시 적은 클래스에 대한 학습을 중요하게 여기게 해서 부족한 클래스에 대한 학습을 높인다.\n",
        "\n",
        "class_counts = train['label'].value_counts(sort=False).to_dict()\n",
        "num_samples = sum(class_counts.values())\n",
        "class_weights = {l: round(num_samples/class_counts[l], 2) for l in class_counts.keys()}\n",
        "\n",
        "print(f'cls_cnts: {len(class_counts)}\\nnum_samples:{num_samples}')\n",
        "labels = train['label'].to_list()\n",
        "weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n",
        "sampler = torch.utils.data.WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\n",
        "\n",
        "cls_cnts: 19\n",
        "num_samples:2938\n",
        "#위의 결과를 보면 클래스 수는 19개라는 것을 알 수있고 (cls_cnts: 19)\n",
        "\n",
        "#아래를 보면 각 클래스별 가중치를 알 수 있다. 부족한 클래스에 대해 높은 가중치가 설정되었다.\n",
        "\n",
        "class_weights\n",
        "\n",
        "{훼손: 2.46,\n",
        " 오염: 5.81,\n",
        " 꼬임: 16.46,\n",
        " 걸레받이수정: 11.29,\n",
        " 석고수정: 60.11,\n",
        " 들뜸: 64.3,\n",
        " 오타공: 24.47,\n",
        " 터짐: 21.43,\n",
        " 곰팡이: 23.84,\n",
        " 창틀,문틀수정: 125.68,\n",
        " 몰딩수정: 26.59,\n",
        " 면불량: 35.0,\n",
        " 피스: 67.44,\n",
        " 울음: 153.61,\n",
        " 녹오염: 251.36,\n",
        " 이음부불량: 197.5,\n",
        " 반점: 1382.5,\n",
        " 가구수정: 276.5,\n",
        " 틈새과다: 691.25}\n",
        "#7. 기존에 사용했던 cross entropy같은 경우는 모든 클래스에 대해 동일한 가중치를 가지고 손실을 계산했다면 이번에는 부족한 class에 높은 가중치를 적용해 계산하는 focal loss를 통해서 평가를 진행하겠다.\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        if self.alpha is not None:\n",
        "            alpha = self.alpha.to(inputs.device)\n",
        "            focal_loss = (alpha[targets] * (1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        else:\n",
        "            focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
        "        return focal_loss\n",
        "def train3(model, optimizer, train_loader, val_loader, scheduler, device):\n",
        "    model.to(device)\n",
        "   #기존에는 criterion = nn.CrossEntropy()를 사용했는데 FocalLoss로 변경했다.\n",
        "    criterion = FocalLoss(alpha=torch.DoubleTensor([class_weights[i] for i in range(len(class_weights))]), gamma=2).to(device)\n",
        "\n",
        "    best_score = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(1, CFG['EPOCHS']+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        for imgs, labels in tqdm(iter(train_loader)):\n",
        "            imgs = imgs.float().to(device)\n",
        "            labels = labels.type(torch.LongTensor).to(device)      # ADDED .type(torch.LongTensor)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(imgs)\n",
        "            loss = criterion(output, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
        "        _train_loss = np.mean(train_loss)\n",
        "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val Weighted F1 Score : [{_val_score:.5f}]')\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(_val_score)\n",
        "\n",
        "        if best_score < _val_score:\n",
        "            best_score = _val_score\n",
        "            best_model = model\n",
        "\n",
        "    return best_model\n",
        "def validation(model, criterion, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    preds, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(iter(val_loader)):\n",
        "            imgs = imgs.float().to(device)\n",
        "            labels = labels.type(torch.LongTensor).to(device)      # ADDED .type(torch.LongTensor)\n",
        "\n",
        "            pred = model(imgs)\n",
        "\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
        "            true_labels += labels.detach().cpu().numpy().tolist()\n",
        "\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        _val_loss = np.mean(val_loss)\n",
        "        _val_score = f1_score(true_labels, preds, average='weighted')\n",
        "\n",
        "    return _val_loss, _val_score\n",
        "#8. 모델은 efficientnet을 발전시킨 EfficientNetV2M 을 사용했다. 이미지 분류에서 상당히 높은 성능을 기록한 모델이고 비교적 용량이 작기 때문에 사용했다.\n",
        "\n",
        "class EfficientNetV2M(nn.Module):\n",
        "    def __init__(self, num_classes=19, pretrained=True):\n",
        "        super(EfficientNetV2M, self).__init__()\n",
        "        self.backbone = timm.create_model('tf_efficientnetv2_m', pretrained=pretrained)\n",
        "        self.backbone.classifier = nn.Linear(self.backbone.classifier.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "modelefv2=EfficientNetV2M()\n",
        "modelefv2.eval()\n",
        "optimizer = torch.opt5. 5im.AdamW(params = modelefv2.parameters(), lr = 3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
        "\n",
        "infer_modelefv2 = train3(modelefv2, optimizer, train_loaderefv2, val_loaderefv2, scheduler, device)\n",
        "#9. 학습한 모델을 저장한다.\n",
        "\n",
        "torch.save(infer_modelefv2, 'C:\\\\Users\\\\82107\\\\Downloads\\\\open (2)\\\\bitm_2_32_30.pt')\n",
        "#10. 성능이 좋다고 알려진 efficientnet_7을 noise 데이터로 학습시킨 'tf_efficientnet_b7_ns' 모델을 이용하여 학습을  진행하였다.\n",
        "\n",
        "       (밑의 과정은 모델명만 바꾸면 되기 때문에 생략하겠다.)\n",
        "\n",
        "class NoisyStudentB7(nn.Module):\n",
        "    def __init__(self, num_classes=19, pretrained=True):\n",
        "        super(NoisyStudentB7, self).__init__()\n",
        "        self.backbone = timm.create_model('tf_efficientnet_b7_ns', pretrained=True)\n",
        "        self.backbone.classifier = nn.Linear(self.backbone.classifier.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x\n",
        "#11. 또다른 모델인 vision_transformer_vit_l/16을 이용해 학습시켰다.\n",
        "\n",
        "       (밑의 과정은 모델명만 바꾸면 되기 때문에 생략하겠다.)\n",
        "\n",
        "class vitl(nn.Module):\n",
        "    def __init__(self, num_classes=19, pretrained=True):\n",
        "        super(vitl, self).__init__()\n",
        "        self.backbone = timm.create_model('vit_large_patch16_384', pretrained=pretrained, num_classes=num_classes)\n",
        "        self.head = nn.Linear(in_features=self.backbone.head.out_features, out_features=num_classes, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    }
  ]
}